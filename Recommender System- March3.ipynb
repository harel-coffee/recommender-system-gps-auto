{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import csv\n",
    "import multiprocessing\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "from annoy import AnnoyIndex\n",
    "import nmslib\n",
    "import tensorflow as tf\n",
    "import bottleneck as bn\n",
    "\n",
    "import heapq\n",
    "import time\n",
    "import operator\n",
    "from collections import Counter,defaultdict\n",
    "from outliers import smirnov_grubbs as grubbs\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score,precision_at_k,recall_at_k\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, roc_curve, auc,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix, coo_matrix, hstack,dok_matrix, lil_matrix, vstack\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim import corpora,models,similarities\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "import itertools\n",
    "import shap\n",
    "import seaborn as sns\n",
    "\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 100\n",
    "#mpl.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH=\"/mnt/projects/continuous_scanning/trackingChange/recData/\"\n",
    "NUM_THREADS=72\n",
    "def make_cdf( data, label=\"\"):\n",
    "\n",
    "    sorted_data = np.sort(data)\n",
    "    yvals=np.arange(len(sorted_data))/float(len(sorted_data)-1)\n",
    "\n",
    "    plt.plot(sorted_data,yvals, label=label,alpha=0.8)\n",
    "    return yvals\n",
    "\n",
    "def cold_train_test_split(interactions,num_users,train_percentage=0.5,random_state=7):\n",
    "    \n",
    "    #remove weird duplicates that are introduced by lightfm\n",
    "    interactions_dok=dok_matrix((interactions.shape),dtype=interactions.dtype)\n",
    "    interactions_dok._update(zip(zip(interactions.row,interactions.col),interactions.data))\n",
    "\n",
    "    train_a = interactions_dok.tocsr()\n",
    "    test_a = interactions_dok.tocsr()\n",
    "    \n",
    "    train_mask = np.array([True]*num_users)\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.choice(np.arange(num_users), replace=False,\n",
    "                           size=int(num_users * train_percentage))\n",
    "    \n",
    "    train_mask[indices] = False\n",
    "    test_mask = ~np.array(train_mask)\n",
    "    \n",
    "    nnz_per_row = np.diff(train_a.indptr)\n",
    "    \n",
    "    #zero out rows\n",
    "    \n",
    "    test_a.data[np.repeat(test_mask, nnz_per_row)] = 0\n",
    "    train_a.data[np.repeat(train_mask, nnz_per_row)] = 0\n",
    "    train_a.eliminate_zeros()\n",
    "    test_a.eliminate_zeros()\n",
    "    \n",
    "\n",
    "    #makes its own shape--need to have it retain old shape....\n",
    "    return train_a,test_a\n",
    "\n",
    "\n",
    "\n",
    "def extractData(f, feat = \"slash20\", train_percentage=0.8, withItemFeats=True,\\\n",
    "                calcWeights = True,weightCap = 2000): \n",
    "    df_csv = pd.read_csv(DATAPATH+f,usecols=[\"ip\",feat,\"p\"])#,\"p1\"])\n",
    "    df_csv = df_csv.fillna(0)\n",
    "\n",
    "    #prune the items, as they are expensive\n",
    "    #if withItemFeats:\n",
    "    #    df_csv[\"minidata\"] = df_csv[\"minidata\"].astype('category')\n",
    "    #    df_csv[[\"minidata\"]] = df_csv[[\"minidata\"]].apply(lambda x: x.cat.codes)\n",
    "\n",
    "    #    df_csv['minidata'] = np.where(~df_csv['minidata'].duplicated(keep=False), 0, df_csv['minidata'])\n",
    "\n",
    "        \n",
    "        \n",
    "    #introduce dataset\n",
    "    dataset = Dataset()\n",
    "    if withItemFeats:\n",
    "        dataset.fit(list(df_csv[\"ip\"]),list(df_csv[\"p\"]),\\\n",
    "        user_features=list(df_csv[feat]),\n",
    "        item_features = list(df_csv[\"p\"])+list(df_csv[\"p1\"]))#+list(df_csv[\"p3\"]))\n",
    "    else:\n",
    "        dataset.fit(df_csv[\"ip\"],df_csv[\"p\"],\\\n",
    "                user_features=df_csv[feat])\n",
    "    #user_features = list(df_csv[\"ip\"])+list(df_csv[\"s1\"])+list(df_csv[\"s2\"])+list(df_csv[\"s3\"])+list(df_csv[\"s4\"])+\n",
    "\n",
    "    \n",
    "    num_users, num_items = dataset.interactions_shape()\n",
    "    print('Num users: {}, num_items {}.'.format(num_users, num_items))\n",
    "\n",
    "\n",
    "    \n",
    "    #get inverse propensity weights\n",
    "    if calcWeights: \n",
    "        normed_counts = (1/(df_csv['p'].value_counts(normalize=True)*100))\n",
    "        df_weights = pd.DataFrame({'p':normed_counts.index, 'w':normed_counts.values})\n",
    "        df_csv= pd.merge(df_csv,df_weights,how='left',on=\"p\")\n",
    "        df_csv['w'] = df_csv['w'].apply(lambda x: weightCap if x > weightCap else x)\n",
    "    \n",
    "        #build interactions\n",
    "        (interactions, weights) = \\\n",
    "        dataset.build_interactions(list(zip(df_csv['ip'], df_csv['p'],df_csv['w'])))\n",
    "    else:\n",
    "        (interactions, weights) = \\\n",
    "        dataset.build_interactions(list(zip(df_csv['ip'], df_csv['p'])))\n",
    "\n",
    "    #print(\"making to dense\")\n",
    "    #interactions = interactions *2\n",
    "    #interactions = coo_matrix(interactions.todense() - 1)\n",
    "    #print(\"success\")\n",
    "    print(repr(interactions))\n",
    "\n",
    "    #build featureset\n",
    "    newFeats = list(zip(df_csv['ip'], \\\n",
    "                        list(zip(list(df_csv[feat])))))\n",
    "    user_features = dataset.build_user_features(newFeats)\n",
    "    \n",
    "\n",
    "\n",
    "    #remove identity matrix since this is a cold start\n",
    "    #user_features.setdiag([0]*num_users)\n",
    "    #user_features.eliminate_zeros()\n",
    "    #user_features = normalize(user_features, norm='l2', axis=1)\n",
    "    \n",
    "    if withItemFeats:\n",
    "        newFeats = list(set(list(zip(df_csv['p'], list(zip(list(df_csv[\"p\"]),list(df_csv[\"p1\"])))))))#,list(df_csv[\"p3\"])))))))\n",
    "\n",
    "        item_features = dataset.build_item_features(newFeats)\n",
    "        ''' \n",
    "        item_features.setdiag([0]*num_items)\n",
    "        item_features.eliminate_zeros()\n",
    "        item_features = normalize(item_features, norm='l1', axis=1)\n",
    "        '''\n",
    "    \n",
    "    #list(zip(list(df_csv[\"asn\"]),list(df_csv[\"s1\"])))\n",
    "    #list(df_csv[\"asn\"]),list(df_csv[\"s2\"]),list(df_csv[\"s3\"]),list(df_csv[\"s4\"]),\n",
    "    #create train test\n",
    "    \n",
    "    train,test = cold_train_test_split(interactions,num_users,train_percentage=train_percentage)\n",
    "\n",
    "    \n",
    "    #handle weight stuff\n",
    "    trainw = train.multiply(weights)\n",
    "    testw = test.multiply(weights)\n",
    "    train.sort_indices()\n",
    "    trainw.sort_indices()\n",
    "    test.sort_indices()\n",
    "    testw.sort_indices()\n",
    "    trainw = trainw.tocoo()\n",
    "    testw = testw.tocoo()\n",
    "    \n",
    "    #zero out rows for training\n",
    "    train_rows = np.array(list(set(trainw.row)))\n",
    "    userf_train_mask = np.array([True]*num_users)\n",
    "    userf_train_mask[train_rows] = False\n",
    "    nnz_per_row = np.diff(user_features.indptr)\n",
    "    userf_train = user_features.copy()\n",
    "    userf_train.data[np.repeat(userf_train_mask, nnz_per_row)] = 0\n",
    "    userf_train.eliminate_zeros()\n",
    "    \n",
    "    #good to have mappings\n",
    "    dmap= dataset.mapping()\n",
    "    inv_ip_map = {v: k for k, v in dmap[0].items()}\n",
    "    inv_port_map = {v: k for k, v in dmap[2].items()}\n",
    "    \n",
    "    if withItemFeats:\n",
    "        return df_csv, dataset, num_users, num_items, interactions, weights,user_features, userf_train, item_features, train,test,trainw,testw, dmap, inv_ip_map, inv_port_map\n",
    "    else:\n",
    "        return df_csv, dataset, num_users, num_items, interactions, weights,user_features, userf_train, None, train,test,trainw,testw,dmap, inv_ip_map, inv_port_map\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up the Data for the Recommendor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"ht65k_miniset.iconv\" # removed null byte! sed 's/\\x0//g' ht65k_1pset.csv > ht65k_1pset.iconv\n",
    "#f = \"ht65k_1pset.iconv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map,\\\n",
    "inv_port_map = extractData(f,train_percentage = 0.8, withItemFeats=False,calcWeights= False)\n",
    "\n",
    "#print(str(df[df[\"ip\"]==\"164.10.45.170\"][\"minidata\"]))\n",
    "print(user_features.shape)\n",
    "#print(item_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Run the Recommendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "                random_state=2016,\n",
    "                learning_rate=0.01, #0.05\n",
    "                no_components=4000,\n",
    "                #learning_schedule='adadelta',\n",
    "                max_sampled= 175, #50,\n",
    "                #user_alpha=0.5\n",
    "                       \n",
    "                      )\n",
    "\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                sample_weight = trainw,\n",
    "                  epochs=10, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = [22003, 41463, 13817, 12794, 44029]\n",
    "user_ids = test_rows[pids]\n",
    "for u,i in zip(user_ids,pids):\n",
    "    print(df[df[\"ip\"] == inv_ip_map[u]][[\"p\"]])\n",
    "    print([inv_port_map[j] for j in predictions[i][0]])\n",
    "    print(predictions[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmarking number of Epochs for Training\")\n",
    "epochs = [2,4,6,8,18,28] #36 seconds per epoch\n",
    "norm_services = [215,276,305,325,477,596]\n",
    "frac_services = [0.33,0.33,.32,.32,.32,.28]\n",
    "frac_ips = [0.5,0.51,.51,.51,.5,.48]\n",
    "\n",
    "plt.plot(epochs,frac_ips, '.-', label=\"Fraction of IPs\")\n",
    "plt.plot(epochs,frac_services,'.-',label=\"Fraction of Services\")\n",
    "plt.plot(epochs,np.array(norm_services)/1000, '.-',label=\"Norm Services *10^4\")\n",
    "plt.xlabel(\"Epochs (35 seconds train/epoch)\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmarking number of Guesses\")\n",
    "num_guesses = [10,20,30,40,50,60,70,80,90,100] #36 seconds per epoch\n",
    "norm_services = [253,367,447,513,566,599,626,651,693,716]\n",
    "frac_services = [0.367,0.412,.438,.453,.463,.467,.468,.478,.471,.483]\n",
    "frac_ips = [0.635,0.666,.693,.711,.721,.723,.726,.737,.725,.743]\n",
    "\n",
    "plt.plot(num_guesses,frac_ips, '.-', label=\"Fraction of IPs\")\n",
    "plt.plot(num_guesses,frac_services,'.-',label=\"Fraction of Services\")\n",
    "plt.plot(num_guesses,np.array(norm_services)/1000, '.-',label=\"Norm Services *10^4\")\n",
    "plt.xlabel(\"Number of Guesses\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "slash23<br>\n",
    "Total Normalized Services:  275.60068175444036<br>\n",
    "Total Fraction Services: 0.1504163978262171<br>\n",
    "Total Fraction IPs: 0.26026281153043646<br>\n",
    "slash22<br>\n",
    "Total Normalized Services:  345.8524479960912<br>\n",
    "Total Fraction Services: 0.19542860373693768<br>\n",
    "Total Fraction IPs: 0.31960831110155413<br>\n",
    "slash20<br>\n",
    "Total Normalized Services:  470.6145427630729<br>\n",
    "Total Fraction Services: 0.30577136699548346<br>\n",
    "Total Fraction IPs: 0.4869636312509895<br>\n",
    "slash19<br>\n",
    "Total Normalized Services:  548.2586308052825<br>\n",
    "Total Fraction Services: 0.3608868741560627<br>\n",
    "Total Fraction IPs: 0.5565077872271044<br>\n",
    "slash18<br>\n",
    "Total Normalized Services:  605.7537606680435<br>\n",
    "Total Fraction Services: 0.39644980942815145<br>\n",
    "Total Fraction IPs: 0.595762217238943   <br>\n",
    "slash17<br>\n",
    "Total Normalized Services:  657.5500775907003<br>\n",
    "Total Fraction Services: 0.44045527714401644<br>\n",
    "Total Fraction IPs: 0.6762068370696113<br>\n",
    "slash16<br>\n",
    "Total Normalized Services:  712.1528322787614<br>\n",
    "Total Fraction Services: 0.4779535310668698<br>\n",
    "Total Fraction IPs: 0.7342791586817362<br>\n",
    "ASN<br>\n",
    "Total Normalized Services:  1031.9949926509712<br>\n",
    "Total Fraction Services: 0.4221605459733798<br>\n",
    "Total Fraction IPs: 0.6154804072846195<br>\n",
    "<br>\n",
    "Max Sampled500, 0.05 LR <br>\n",
    "ASN or slash16 (whichever smaller)<br>\n",
    "Total Normalized Services:  788.1392639226955<br>\n",
    "Total Normalized IPs:  47579.61659849398<br>\n",
    "Total Fraction Services: 0.2771394698093774<br>\n",
    "Total Fraction IPs: 0.39957797033504805<br>\n",
    "\n",
    "ASN<br>\n",
    "Total Normalized Services:  742.5351331088734<br>\n",
    "Total Normalized IPs:  47807.5798893569<br>\n",
    "Total Fraction Services: 0.28212160527192265<br>\n",
    "Total Fraction IPs: 0.4109968737668156<br>\n",
    "slash16<br>\n",
    "Total Normalized Services:  579.7953558116823<br>\n",
    "Total Normalized IPs:  49038.62965705219<br>\n",
    "Total Fraction Services: 0.2863692242114928<br>\n",
    "Total Fraction IPs: 0.39370263685836016<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning Rate of 0.05, max sampled 500\")\n",
    "retrains = [False] \n",
    "retrain_weights = [False] \n",
    "BENCH2_CYCLES = 30\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = False\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = True\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=600\n",
    "\n",
    "total_epochs = 2\n",
    "\n",
    "#for r,rw in zip(retrains,retrain_weights):\n",
    "for curFeat in [\"asn\",\"slash16\"]: #,\n",
    "    print(curFeat)\n",
    "\n",
    "    del hybrid_model\n",
    "    del user_embeddings\n",
    "    del item_embeddings\n",
    "    del df\n",
    "    gc.collect()\n",
    "    #''' \n",
    "    f = \"ht65k_1pset.iconv\"\n",
    "    df, dataset, num_users, num_items, interactions, weights, \\\n",
    "    user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "    inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.08, withItemFeats=False,calcWeights= False)\n",
    "\n",
    "\n",
    "    hybrid_model = LightFM(loss='warp', #'warp\n",
    "        random_state=2016,\n",
    "        learning_rate=0.05,\n",
    "        no_components=4000,\n",
    "        #learning_schedule='adadelta',\n",
    "        max_sampled= 500, #175,\n",
    "        #user_alpha=0.5\n",
    "\n",
    "        )\n",
    "\n",
    "    \n",
    "    print(\"Initialized Model\")\n",
    "    hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                      user_features=userf_train,\n",
    "                      #item_features=item_features,\n",
    "                    #sample_weight = trainw,\n",
    "                      epochs=20, #10\n",
    "                      num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "   \n",
    "    print(\"EF_CONSTRUCTION: \", EF_CONSTRUCTION)\n",
    "    model_description = constructDescript(\"PlayingOptimized_\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=True)\n",
    "    print(model_description)\n",
    "\n",
    "    \n",
    "    START = 0\n",
    "    LIM = 4539900\n",
    "    NUM_PREDS = 100\n",
    "\n",
    "\n",
    "    num_test_ips, num_services,num_ips_per_port, test_rows, testcoo,\\\n",
    "        item_biases, item_embeddings, user_biases, user_embeddings, predictions, \\\n",
    "        pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "        prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                     item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                     NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "    #'''\n",
    "    all_correctly_predicted, norm_services, frac_service, frac_ip, total_ips_correct \\\n",
    "    = evalSuccess(predictions,test,dmap,num_ips_per_port, num_services,num_test_ips)\n",
    "\n",
    "    #np.save(DATAPATH + \"1p_train20Epoch_100Pred_600EF_\" + curFeat+ \"_predictions.npy\",all_correctly_predicted)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correctly_predicted, norm_services, frac_service, frac_ip, total_ips_correct \\\n",
    "    = evalSuccess(predictions,test,dmap,num_ips_per_port, num_services,num_test_ips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False Retrain\")\n",
    "\n",
    "BENCH2_CYCLES = 100\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = False\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = True\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=600\n",
    "\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 100\n",
    "\n",
    "\n",
    "\n",
    "bench2_results,predictions,all_correctly_predicted =  bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,WITH_ITEM_FEATS,model_description)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Retrain\")\n",
    "\n",
    "\n",
    "BENCH2_CYCLES = 100\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = True\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = True\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=600\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 100\n",
    "\n",
    "'''\n",
    "curFeat = \"slash16\"\n",
    "f = \"ht65k_1pset.iconv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.08, withItemFeats=False,calcWeights= False)\n",
    "\n",
    "\n",
    "\n",
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "    random_state=2016,\n",
    "    learning_rate=0.05, #0.05\n",
    "    no_components=4000,\n",
    "    #learning_schedule='adadelta',\n",
    "    max_sampled= 2000, #50,\n",
    "    #user_alpha=0.5\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Initialized Model\")\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                #sample_weight = trainw,\n",
    "                  epochs=20, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "model_description = constructDescript(\"Epoch20_maxsample2000_slash16user_1p_model_\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=True)\n",
    "print(model_description)\n",
    "\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows, testcoo,\\\n",
    "    item_biases, item_embeddings, user_biases, user_embeddings, predictions, \\\n",
    "    pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "    prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                 item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                 NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "\n",
    "#'''\n",
    "\n",
    "bench2_results,predictions,all_correctly_predicted = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,WITH_ITEM_FEATS,model_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del inv_ip_map\n",
    "#del user_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Retrain,slash16,more filtered dataset\")\n",
    "\n",
    "\n",
    "BENCH2_CYCLES = 100\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = True\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = True\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=600\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 100\n",
    "\n",
    "RETRAIN_GAP = 10\n",
    "RETRAIN_ALL = True\n",
    "RETRAIN_EPOCHS =10 \n",
    "\n",
    "'''\n",
    "curFeat = \"slash16\"\n",
    "#f = \"ht65k_1pset.iconv\"\n",
    "f = \"ht65k_1pset_l10.iconv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.08, withItemFeats=False,calcWeights= False)\n",
    "\n",
    "\n",
    "\n",
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "    random_state=2016,\n",
    "    learning_rate=0.05, #0.05\n",
    "    no_components=4000,\n",
    "    #learning_schedule='adadelta',\n",
    "    max_sampled= 175,\n",
    "    #user_alpha=0.5\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Initialized Model\")\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                #sample_weight = trainw,\n",
    "                  epochs=20, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, predictions, \\\n",
    "    pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "    prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                 item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                 NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "\n",
    "'''\n",
    "model_description = constructDescript(\"PlayingRetrainGap10_6KComp_\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=True)\n",
    "print(model_description)\n",
    "bench2_results,predictions,all_correctly_predicted = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION,model_description)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Retrain,slash16,more filtered dataset, retrain all false\")\n",
    "\n",
    "\n",
    "\n",
    "BENCH2_CYCLES = 600\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = True\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = True\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=600\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 100\n",
    "\n",
    "RETRAIN_GAP = 10\n",
    "RETRAIN_ALL = False\n",
    "RETRAIN_EPOCHS =10 \n",
    "\n",
    "#'''\n",
    "curFeat = \"slash16\"\n",
    "#f = \"ht65k_1pset.iconv\"\n",
    "f = \"ht65k_1pset_l10.iconv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.08, withItemFeats=False,calcWeights= False)\n",
    "#'''\n",
    "\n",
    "\n",
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "    random_state=2016,\n",
    "    learning_rate=0.05, #0.05\n",
    "    no_components=4000,\n",
    "    #learning_schedule='adadelta',\n",
    "    max_sampled= 175,\n",
    "    #user_alpha=0.5\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Initialized Model\")\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                #sample_weight = trainw,\n",
    "                  epochs=20, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, predictions, \\\n",
    "    pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "    prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                 item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                 NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "\n",
    "#'''\n",
    "model_description = constructDescript(\"PlayingRetrainGap10_6KComp_RetrainAllFalse\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=True)\n",
    "print(model_description)\n",
    "bench2_results,predictions,all_correctly_predicted = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION,model_description)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Retrain,slash16,more filtered dataset, retrain all false\")\n",
    "\n",
    "\n",
    "BENCH2_CYCLES = 300\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = True\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = True\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=600\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 100\n",
    "\n",
    "RETRAIN_GAP = 10\n",
    "RETRAIN_ALL = False\n",
    "RETRAIN_EPOCHS =10 \n",
    "\n",
    "#'''\n",
    "curFeat = \"slash16\"\n",
    "#f = \"ht65k_1pset.iconv\"\n",
    "f = \"ht65k_1pset_l10.iconv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.08, withItemFeats=False,calcWeights= False)\n",
    "#'''\n",
    "\n",
    "\n",
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "    random_state=2016,\n",
    "    learning_rate=0.01, #0.05\n",
    "    no_components=4000,\n",
    "    #learning_schedule='adadelta',\n",
    "    max_sampled= 175,\n",
    "    #user_alpha=0.5\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Initialized Model\")\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                #sample_weight = trainw,\n",
    "                  epochs=20, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, predictions, \\\n",
    "    pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "    prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                 item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                 NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "\n",
    "#'''\n",
    "model_description = constructDescript(\"PlayingRetrainGap10_4KComp_0.01LR_RetrainAllFalse\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=True)\n",
    "print(model_description)\n",
    "bench2_results,predictions,all_correctly_predicted = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION,model_description)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hybrid_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Retrain,slash16,more filtered dataset, retrain all false, EF_Construction 1K\")\n",
    "\n",
    "\n",
    "BENCH2_CYCLES = 300\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = True\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = True\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=1000\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 100\n",
    "\n",
    "RETRAIN_GAP = 10\n",
    "RETRAIN_ALL = False\n",
    "RETRAIN_EPOCHS =10 \n",
    "\n",
    "#'''\n",
    "curFeat = \"slash16\"\n",
    "#f = \"ht65k_1pset.iconv\"\n",
    "f = \"ht65k_1pset_l10.iconv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.08, withItemFeats=False,calcWeights= False)\n",
    "#'''\n",
    "\n",
    "\n",
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "    random_state=2016,\n",
    "    learning_rate=0.01, #0.05\n",
    "    no_components=4000,\n",
    "    #learning_schedule='adadelta',\n",
    "    max_sampled= 175,\n",
    "    #user_alpha=0.5\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Initialized Model\")\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                #sample_weight = trainw,\n",
    "                  epochs=20, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, predictions, \\\n",
    "    pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "    prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                 item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                 NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "\n",
    "#'''\n",
    "model_description = constructDescript(\"PlayingRetrainGap10_4KComp_0.01LR_RetrainAllFalse_EF1K\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=True)\n",
    "print(model_description)\n",
    "bench2_results,predictions,all_correctly_predicted = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION,model_description)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hybrid_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"True Retrain,slash16,more filtered dataset, retrain all false, EF_Construction 1K\")\n",
    "\n",
    "\n",
    "BENCH2_CYCLES = 300\n",
    "\n",
    "\n",
    "RETRAIN = True\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = False\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=1000\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 100\n",
    "\n",
    "RETRAIN_GAP = 10\n",
    "RETRAIN_ALL = False\n",
    "RETRAIN_EPOCHS =10 \n",
    "\n",
    "#'''\n",
    "curFeat = \"slash16\"\n",
    "#f = \"ht65k_1pset.iconv\"\n",
    "f = \"ht65k_1pset_l10.iconv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.08, withItemFeats=False,calcWeights= False)\n",
    "#'''\n",
    "\n",
    "\n",
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "    random_state=2016,\n",
    "    learning_rate=0.01, #0.05\n",
    "    no_components=4000,\n",
    "    #learning_schedule='adadelta',\n",
    "    max_sampled= 175,\n",
    "    #user_alpha=0.5\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Initialized Model\")\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                #sample_weight = trainw,\n",
    "                  epochs=20, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, predictions, \\\n",
    "    pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "    prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                 item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                 NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "\n",
    "#'''\n",
    "model_description = constructDescript(\"PlayingRetrainGap10_4KComp_0.01LR_RetrainAllFalse_EF1K\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=BIASED)\n",
    "print(model_description)\n",
    "bench2_results,predictions,all_correctly_predicted = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION,model_description)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del hybrid_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Retrain,slash16,more filtered dataset, retrain all false, EF_Construction 1K,Calc Weights\")\n",
    "\n",
    "\n",
    "BENCH2_CYCLES = 300\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = True\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = False\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=1000\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 100\n",
    "\n",
    "RETRAIN_GAP = 10\n",
    "RETRAIN_ALL = False\n",
    "RETRAIN_EPOCHS =10 \n",
    "      \n",
    "CALC_WEIGHTS = True\n",
    "\n",
    "'''\n",
    "curFeat = \"slash16\"\n",
    "#f = \"ht65k_1pset.iconv\"\n",
    "f = \"ht65k_1pset_l10.iconv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.08, withItemFeats=False,calcWeights= CALC_WEIGHTS)\n",
    "\n",
    "\n",
    "\n",
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "    random_state=2016,\n",
    "    learning_rate=0.01, #0.05\n",
    "    no_components=4000,\n",
    "    #learning_schedule='adadelta',\n",
    "    max_sampled= 175,\n",
    "    #user_alpha=0.5\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Initialized Model\")\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                  sample_weight = trainw,\n",
    "                  epochs=20, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, predictions, \\\n",
    "    pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "    prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                 item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                 NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "\n",
    "'''\n",
    "model_description = constructDescript(\"PlayingRetrainGap10_4KComp_0.01LR_RetrainAllFalse_EF1K_CalcWeightsTrue\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=BIASED)\n",
    "print(model_description)\n",
    "bench2_results,predictions,all_correctly_predicted = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,testw,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,CALC_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION,model_description)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True Retrain, UNIFORM Sample  and Oversample0.64 Train,slash16,more filtered dataset, retrain all false, EF_Construction 1K,10kCalc Weights\")\n",
    "\n",
    "\n",
    "BENCH2_CYCLES = 300\n",
    "BIASED=True\n",
    "\n",
    "\n",
    "RETRAIN = True\n",
    "RETRAIN_WEIGHTS = False\n",
    "WITH_ITEM_FEATS=False\n",
    "BIASED = False\n",
    "HIST_BINS=100\n",
    "EF_CONSTRUCTION=1000\n",
    "\n",
    "START = 0\n",
    "LIM = 4539900\n",
    "NUM_PREDS = 300\n",
    "\n",
    "RETRAIN_GAP = 10\n",
    "RETRAIN_ALL = False\n",
    "RETRAIN_EPOCHS =10 \n",
    "      \n",
    "CALC_WEIGHTS = True\n",
    "\n",
    "#'''\n",
    "curFeat = \"asn\"\n",
    "f = \"ht65k_1pset_uniform40.csv\"\n",
    "df, dataset, num_users, num_items, interactions, weights, \\\n",
    "user_features,userf_train, item_features, train,test,trainw,testw,dmap, inv_ip_map, \\\n",
    "inv_port_map = extractData(f,feat = curFeat, train_percentage = 0.1, \\\n",
    "                           withItemFeats=False,calcWeights= CALC_WEIGHTS,\\\n",
    "                          weightCap = 10000)\n",
    "\n",
    "\n",
    "\n",
    "hybrid_model = LightFM(loss='warp', #'warp\n",
    "    random_state=2016,\n",
    "    learning_rate=0.01, #0.05\n",
    "    no_components=4000,\n",
    "    #learning_schedule='adadelta',\n",
    "    max_sampled= 175,\n",
    "    #user_alpha=0.5\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Initialized Model\")\n",
    "hybrid_model = hybrid_model.fit(train, #newtrain\n",
    "                  user_features=userf_train,\n",
    "                  #item_features=item_features,\n",
    "                  sample_weight = trainw,\n",
    "                  epochs=20, #10\n",
    "                  num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "#'''\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, predictions, \\\n",
    "    pred_ranks, testPorts, hist_preds,LIM = \\\n",
    "    prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                 item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS,\\\n",
    "                 NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION )\n",
    "\n",
    "\n",
    "model_description = constructDescript(\"0.1Train_uniform40_PlayingRetrainGap10_4KComp_0.01LR_RetrainAllFalse_EF1K_10kCalcWeightsTrue_ASN_\", RETRAIN =RETRAIN, RETRAIN_WEIGHTS =  RETRAIN_WEIGHTS, BIASED=BIASED)\n",
    "print(model_description)\n",
    "bench2_results,predictions,all_correctly_predicted = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                           test,testw,test_rows,train,user_features,item_features, \\\n",
    "                                           NUM_THREADS,RETRAIN,RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                           RETRAIN_WEIGHTS,CALC_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION,model_description)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(df, df[\"p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATAPATH + \"ht65k_1pset_uniform200_l10_oversample_fakeIPs.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hit = convert(all_correctly_predicted)\n",
    "df_hit.to_csv(DATAPATH + \"0.64Train_uniform200_l10_corr_predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hybrid_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcNormed(all_correctly_predicted,num_ips_per_port):\n",
    "    num_correctly_pred_per_port = coo_matrix(all_correctly_predicted.sum(axis=0))\n",
    "\n",
    "    #do some kind of sum of all of these fractions and divide by total number ports?\n",
    "    norm_services = coo_matrix(num_correctly_pred_per_port/num_ips_per_port)\n",
    "    norm_services.data = np.nan_to_num(norm_services.data, copy=False)\n",
    "    norm_services = norm_services.sum()\n",
    "    return norm_services\n",
    "\n",
    "def evalSuccess(predictions,test,dmap,num_ips_per_port, num_services,num_test_ips):\n",
    "\n",
    "    \n",
    "    predictions_identity = coo_matrix(([1] * len(predictions.data),\\\n",
    "                                                  (predictions.tocoo().row, predictions.tocoo().col)),\\\n",
    "                                                   shape=test.shape)\n",
    "\n",
    "\n",
    "    all_correctly_predicted = (test.multiply(predictions_identity)).tocoo()\n",
    "    \n",
    "    del predictions_identity\n",
    "    corPred_identityish = coo_matrix(([1] * len(set(all_correctly_predicted.row)),\\\n",
    "                                                  (list(set(all_correctly_predicted.row)), list(set(all_correctly_predicted.row)))),\\\n",
    "                                                   shape=(test.shape[0],test.shape[0]))\n",
    "\n",
    "\n",
    "    #extracting rows w/ at least 1 prediction, pretending we found all ports\n",
    "    normalized_found = corPred_identityish * test\n",
    "    del corPred_identityish\n",
    "\n",
    "    total_services_correct = all_correctly_predicted.count_nonzero()\n",
    "    total_ips_correct = len(set(all_correctly_predicted.row))\n",
    "    print(\"total ips correct: \",total_ips_correct)    \n",
    "\n",
    "    norm_services = calcNormed(all_correctly_predicted,num_ips_per_port)\n",
    "    norm_IPs = calcNormed(normalized_found, num_ips_per_port)\n",
    "\n",
    "    \n",
    "    norm_services = calcNormed(all_correctly_predicted,num_ips_per_port)\n",
    "    norm_IPs = calcNormed(normalized_found, num_ips_per_port)\n",
    "\n",
    "\n",
    "    #Remove top 4 ports 80,443,7547,22\n",
    "    popPorts = []\n",
    "    for p in [80,443,7547,22]:\n",
    "        popPorts.append(dmap[2][p])\n",
    "    \n",
    "    \n",
    "    popCrew = all_correctly_predicted.tocsr()[:,popPorts]\n",
    "    popCrew_services_correct = popCrew.count_nonzero()\n",
    "    popCrew_ips_correct = len(set(popCrew.tocoo().row))\n",
    "\n",
    "\n",
    "    num_test_ips_depop = len(set(test[:,popPorts].tocoo().row))\n",
    "    num_test_services_depop = test[:,popPorts].count_nonzero()\n",
    "\n",
    "\n",
    "    #if RETRAIN_WEIGHTS:\n",
    "    #    norm_services = norm_services/10\n",
    "\n",
    "    frac_services = total_services_correct/num_services\n",
    "    frac_ips = total_ips_correct/num_test_ips\n",
    "\n",
    "    frac_services_depop = (total_services_correct -popCrew_services_correct) / (num_services - num_test_services_depop)\n",
    "    frac_ips_depop = (total_ips_correct -  popCrew_ips_correct)/(num_test_ips - num_test_ips_depop)\n",
    "\n",
    "\n",
    "    print(\"Total Normalized Services: \", norm_services)\n",
    "    print(\"Total Normalized IPs: \", norm_IPs)\n",
    "    print(\"Total Fraction Services:\", frac_services)\n",
    "    print(\"Total Fraction IPs:\", frac_ips )\n",
    "    print(\"Total Fraction Depopularized Services:\", frac_services_depop)\n",
    "    print(\"Total Fraction Depopularized IPs:\", frac_ips_depop )\n",
    "    \n",
    "    \n",
    "    return all_correctly_predicted, norm_services, frac_services, frac_ips, total_ips_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def groundTruthServices(test,LIM,START):\n",
    "    testcoo = test.tocoo()\n",
    "    test_rows = np.array(list(set(testcoo.row))[START:LIM])\n",
    "    test_rows.sort()\n",
    "    \n",
    "    #adjust lim\n",
    "    if LIM > len(test_rows):\n",
    "        LIM = len(test_rows)\n",
    "    \n",
    "    num_test_ips = len(test_rows)\n",
    "    num_services = sum(test[test_rows].tocoo().data)\n",
    "    num_ips_per_port=  coo_matrix(test[test_rows].sum(axis=0))\n",
    "    return num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, LIM\n",
    "\n",
    "def getNewPredictions(model,user_features,test_rows,item_features,\\\n",
    "                      BIASED,WITH_ITEM_FEATS,OLD=False, NUM_PREDS=30, \\\n",
    "                      EF_CONSTRUCTION = 200,M=20):\n",
    "   \n",
    "    if WITH_ITEM_FEATS:\n",
    "        item_biases, item_embeddings = model.get_item_representations(item_features)\n",
    "    else:\n",
    "        item_biases, item_embeddings = model.get_item_representations()\n",
    "        \n",
    "    print(\"Got Item Representations\")\n",
    "    user_biases, user_embeddings = model.get_user_representations(user_features[test_rows])\n",
    "    print(\"Got User Representations\")\n",
    "    predictions = None\n",
    "    if OLD: \n",
    "        if BIASED:\n",
    "            predictions = (\n",
    "                user_embeddings.dot(item_embeddings.T) \n",
    "                +item_biases.reshape(1, -1) + user_biases.reshape(-1, 1)\n",
    "            ) \n",
    "        else:        \n",
    "            predictions = (\n",
    "                user_embeddings.dot(item_embeddings.T) \n",
    "            ) \n",
    "    else:\n",
    "    \n",
    "        norms = np.linalg.norm(item_embeddings, axis=1)\n",
    "        max_norm = norms.max()\n",
    "        extra_dimension = np.sqrt(max_norm ** 2 - norms ** 2)\n",
    "        norm_data = np.append(item_embeddings, extra_dimension.reshape(norms.shape[0], 1), axis=1)\n",
    "\n",
    "        #first an nmslib\n",
    "        nms_member_idx = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "       \n",
    "        nms_member_idx.addDataPointBatch(norm_data)\n",
    "        \n",
    "        #indexTimeParams = {'M': M, 'indexThreadQty': NUM_THREADS,\\\n",
    "        #                  'efConstruction': efC, 'post' : 0}\n",
    "        #https://github.com/nmslib/nmslib/blob/master/manual/methods.md\n",
    "        indexTimeParams = {'efConstruction':  EF_CONSTRUCTION, 'M':M }\n",
    "        \n",
    "        \n",
    "        nms_member_idx.createIndex(indexTimeParams,print_progress=True)\n",
    "        print(\"Made Index\")\n",
    "    \n",
    "        #allUsers = np.c_[user_embeddings,[0]*len(test_rows)]\n",
    "        allUsers = user_embeddings\n",
    "        top_items = np.array(nms_member_idx.knnQueryBatch(allUsers, k=NUM_PREDS, num_threads=NUM_THREADS))\n",
    "        print(\"Got Top Items\")\n",
    "        del  item_embeddings\n",
    "        del user_embeddings\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        num_r = [len(x) for x in top_items[:,1]]\n",
    "        top_items_coo = coo_matrix((np.concatenate(top_items[:,1]),\\\n",
    "                                  (np.repeat(test_rows, num_r), np.concatenate(top_items[:,0]))),\\\n",
    "                                   shape=test.shape)\n",
    "        if BIASED:\n",
    "            norm = np.linalg.norm(item_biases)\n",
    "            item_biases_norm = (item_biases/norm) #*4\n",
    "            item_biases_coo = coo_matrix((item_biases_norm,\\\n",
    "                                         ([0]*len(item_biases_norm),np.arange(len(item_biases_norm)))))\n",
    "            \n",
    "            top_items_identity = coo_matrix(([1] * len(np.concatenate(top_items[:,1])),\\\n",
    "                                              (np.repeat(test_rows, num_r), np.concatenate(top_items[:,0]))),\\\n",
    "                                               shape=test.shape)\n",
    "            item_biases_identity = top_items_identity.multiply(item_biases_coo)\n",
    "            top_items_biased = top_items_coo - item_biases_identity\n",
    "            top_items = top_items_biased\n",
    "\n",
    "        else:\n",
    "            top_items = top_items_coo\n",
    "    print(\"Got Predictions/Top Items\")\n",
    "    \n",
    "    \n",
    "    return predictions, top_items\n",
    "    \n",
    "\n",
    "def prepTheModel(model,test,train,LIM,START,HIST_BINS, user_features,item_features,\\\n",
    "                 BIASED = True,WITH_ITEM_FEATS=False,NUM_PREDS=30,EF_CONSTRUCTION=200, M=20 ):\n",
    "\n",
    "    \n",
    "    num_test_ips, num_services,num_ips_per_port, test_rows, testcoo, LIM = \\\n",
    "    groundTruthServices(test,LIM,START)\n",
    "    testPorts = list(set(testcoo.col))\n",
    "    \n",
    "    print(\"...Grabbed Representations\")\n",
    "    predictions,  top_items = \\\n",
    "    getNewPredictions(model,user_features,test_rows,item_features,BIASED,\\\n",
    "                      WITH_ITEM_FEATS,NUM_PREDS=NUM_PREDS,EF_CONSTRUCTION =EF_CONSTRUCTION, M=M,OLD=False)\n",
    "    print(\"...Calculated Predictions\")\n",
    "    if predictions is not None:\n",
    "\n",
    "    \n",
    "        #get ranking order of all ports per ip\n",
    "        #pred_ranks = predictions.argsort(axis=-1)[:,::-1]\n",
    "        b = tf.argsort(predictions,axis=-1,direction='DESCENDING',stable=False,name=None)\n",
    "        pred_ranks = tf.keras.backend.eval(b)\n",
    "    \n",
    "        print(\"...Sorted Rankings\")\n",
    "\n",
    "    \n",
    "        hist_preds = hist_scores_per_user(predictions,HIST_BINS)\n",
    "        print(\"...Histogramed the Predictions\")\n",
    "    else: \n",
    "        predictions = top_items\n",
    "        pred_ranks = None\n",
    "        hist_preds = None\n",
    "    return num_test_ips, num_services,num_ips_per_port, test_rows, testcoo,\\\n",
    "        predictions, \\\n",
    "        pred_ranks, testPorts, hist_preds,LIM\n",
    "\n",
    "def constructDescript(prefix,LR=0.01,N_COMP=4000,EPOCHS=10,MAX_SAMPLE=175,BIASED=False,\\\n",
    "                 RETRAIN=False,RETRAIN_WEIGHTS=False,WITH_ITEM_FEATS=False,\\\n",
    "                      CALC_WEIGHTS=False):\n",
    "    \n",
    "    return prefix + str(LR)+\"LR_\"+str(N_COMP)+\"NCOMP_\"+str(EPOCHS)+\"EPOCHS_\" + str(MAX_SAMPLE)+\"MAXSAMPLE_\"\\\n",
    "            +str(BIASED)+\"BIASED_\"\\\n",
    "            + str(RETRAIN)+\"RETRAIN_\"+str(RETRAIN_WEIGHTS)+\"RETRAINWEIGHTS_\"\\\n",
    "            + str(WITH_ITEM_FEATS)+\"WITHITEMFEATS_\" + str(CALC_WEIGHTS)+\"CALCWEIGHTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN!!\n",
    "#testing biased. w/item feats, calc weights, no retrain or retrain weights\n",
    "biases = [True,False]\n",
    "item_feats = [False,False]\n",
    "calc_weights = [False,True]\n",
    "train_percentage = 0.08 #0.8 of a 0.1% scan equivalent\n",
    "\n",
    "\n",
    "for b,i,c in zip(biases,item_feats,calc_weights):\n",
    "    model_description = constructDescript(prefix = \"1p_\", BIASED = b, WITH_ITEM_FEATS=i,CALC_WEIGHTS=c,\\\n",
    "                                           RETRAIN = False,RETRAIN_WEIGHTS=False)\n",
    "    print(model_description)\n",
    "    completeTest(\"ht65k_1pset.iconv\",train_percentage, model_description,BIASED = b, WITH_ITEM_FEATS=i,CALC_WEIGHTS=c,\\\n",
    "                                           RETRAIN = False,RETRAIN_WEIGHTS=False)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bench 2 Implementation: Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bench2_Improvements(model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                        test,testw,test_rows,train,user_features,\\\n",
    "                        item_features, NUM_THREADS,RETRAIN,RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS, BIASED,LIM,HIST_BINS, \\\n",
    "                        CYCLES,RETRAIN_WEIGHTS, CALC_WEIGHTS, WITH_ITEM_FEATS,EF_CONSTRUCTION,model_description):\n",
    "        \n",
    "\n",
    "    print(\"-------\")\n",
    "    print(\"...Beginning Bench2: Compare Different Metrics of Coverage and Hitrate\")\n",
    "    \n",
    "    \"\"\" \n",
    "    bench2_results = calcCoverage(model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                  test,train,user_features,item_features, NUM_THREADS,\\\n",
    "                                  RETRAIN, BIASED,LIM,HIST_BINS, CYCLES,RETRAIN_WEIGHTS,\\\n",
    "                                 WITH_ITEM_FEATS)\n",
    "    \"\"\"\n",
    "    bench2_results,predictions,all_correctly_predicted = calcCoverage_optimized(model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                  test,testw,test_rows,train,user_features,item_features, NUM_THREADS,\\\n",
    "                                  RETRAIN,RETRAIN_GAP, RETRAIN_ALL, RETRAIN_EPOCHS ,BIASED,LIM,HIST_BINS, CYCLES,RETRAIN_WEIGHTS,\\\n",
    "                                 CALC_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION)\n",
    "    print(\"...calculations complete\")\n",
    "    \n",
    "    np.save(DATAPATH + model_description+ \"_coverageB2.npy\",bench2_results)\n",
    "    np.save(DATAPATH + model_description+ \"_correctpredictions.npy\",all_correctly_predicted)\n",
    "    \n",
    "    plotDiscovery(DATAPATH,bench2_results[\"frac_services\"], bench2_results[\"frac_ips\"])\n",
    "\n",
    "    plotHitrate(bench2_results[\"hitrate\"],LIM)\n",
    "\n",
    "    plotNormServiceDiscovery(bench2_results[\"scanned_num\"],bench2_results[\"normed_services\"],LIM)\n",
    "    \n",
    "    plotGradientNormServiceDiscovery(bench2_results[\"scanned_num\"],bench2_results[\"normed_services\"],LIM)\n",
    " \n",
    "    print(\"...Finished Bench2\")\n",
    "    print(\"-------\")\n",
    "    return bench2_results,predictions,all_correctly_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix\n",
    "def top_n_idx_sparse(matrix, n):\n",
    "    '''Return index of top n values in each row of a sparse matrix'''\n",
    "    top_n_idx = []\n",
    "    for le, ri in zip(matrix.indptr[:-1], matrix.indptr[1:]):\n",
    "        n_row_pick = min(n, ri - le)\n",
    "        top_n_idx.append(matrix.indices[le + np.argpartition(matrix.data[le:ri], -n_row_pick)[-n_row_pick:]])\n",
    "    return top_n_idx\n",
    "\n",
    "\n",
    "#OPTIMIZED\n",
    "def calcCoverage_optimized(model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                 test, testw,test_rows,train,user_features, item_features, NUM_THREADS,RETRAIN,\\\n",
    "                 RETRAIN_GAP, RETRAIN_ALL,RETRAIN_EPOCHS, BIASED,LIM,HIST_BINS,\\\n",
    "                 CYCLES,RETRAIN_WEIGHTS,CALC_WEIGHTS,WITH_ITEM_FEATS,EF_CONSTRUCTION):\n",
    "\n",
    "    \n",
    "    bench2_results = {}\n",
    "    portsScanned = {}\n",
    "    normed_services = []\n",
    "    frac_services = []\n",
    "    frac_ips = []\n",
    "    hitrate = []\n",
    "    scanned_num =  []\n",
    "    num_unique_rec = []\n",
    "    pred_all_past = None\n",
    "    start_choices = 1\n",
    "    newtrain = train.tocoo()\n",
    "    total_services_correct = 0\n",
    "    \n",
    "    i = 0                 \n",
    "    while i < CYCLES: \n",
    "        print(\"cycle: \",i)\n",
    "\n",
    "        #TODO:figure out how to handle this\n",
    "        #if RETRAIN:\n",
    "\n",
    "        pred_all_past,  pred_correct, newtrain,minitrain,cur_hitrate,correct, \\\n",
    "        portsScanned,start_choices,scanned_num,predictions = \\\n",
    "        getRekd_v3Optimized(model,portsScanned, predictions, hist_preds, test,test_rows,\\\n",
    "                   newtrain,start_choices,scanned_num,RETRAIN_GAP=RETRAIN_GAP,n=i,LIM=LIM,\\\n",
    "                            HIST_BINS=HIST_BINS,pred_past=pred_all_past)\n",
    "\n",
    "             \n",
    "\n",
    "        print(\"At Bin: \", i)\n",
    "        print(\"Hitrate: \", cur_hitrate)\n",
    "        i = i + RETRAIN_GAP\n",
    "\n",
    "        all_correctly_predicted, norm_services, frac_service, frac_ip, total_ips_correct \\\n",
    "        = evalSuccess(newtrain,test,dmap,num_ips_per_port, num_services,num_test_ips)\n",
    "\n",
    "\n",
    "        normed_services.append(norm_services)\n",
    "        frac_services.append(frac_service)\n",
    "        frac_ips.append(frac_ip)\n",
    "        hitrate.append(cur_hitrate)\n",
    "        num_unique_rec.append(len(portsScanned))\n",
    "        \n",
    "        if RETRAIN:\n",
    "\n",
    "            if RETRAIN_ALL:\n",
    "                training = newtrain\n",
    "            else:\n",
    "                training = minitrain\n",
    "\n",
    "\n",
    "            if RETRAIN_WEIGHTS:\n",
    "                training.multiply(10)\n",
    "                \n",
    "            if CALC_WEIGHTS:\n",
    "                trainingw = training.multiply(testw).tocoo()\n",
    "            else:\n",
    "                trainingw = training.tocoo()    \n",
    "\n",
    "            if WITH_ITEM_FEATS:\n",
    "                model = model.fit_partial(training, \n",
    "                    sample_weight = trainingw, #what to do about the weight???\n",
    "                    user_features=user_features,\n",
    "                    item_features=item_features,\n",
    "                    epochs=RETRAIN_EPOCHS,\n",
    "                    num_threads=NUM_THREADS, verbose=True)\n",
    "            else:\n",
    "                model = model.fit_partial(training,\n",
    "                    sample_weight = trainingw,\n",
    "                    user_features=user_features,\n",
    "                    epochs=RETRAIN_EPOCHS,\n",
    "                    num_threads=NUM_THREADS, verbose=True)       \n",
    "\n",
    "            print(\"...Grabbed Representations\")\n",
    "            _,predictions = \\\n",
    "            getNewPredictions(model,user_features,test_rows,item_features,BIASED,\\\n",
    "                              WITH_ITEM_FEATS,NUM_PREDS=NUM_PREDS, OLD=False,\\\n",
    "                              EF_CONSTRUCTION=EF_CONSTRUCTION)\n",
    "\n",
    "            print(\"...Calculated Predictions\")\n",
    "\n",
    "    bench2_results[\"num_unique_rec\"] = num_unique_rec\n",
    "    bench2_results[\"frac_services\"] = frac_services\n",
    "    bench2_results[\"frac_ips\"] = frac_ips\n",
    "    bench2_results[\"hitrate\"] = hitrate\n",
    "    bench2_results[\"normed_services\"] = normed_services\n",
    "    bench2_results[\"scanned_num\"] = scanned_num\n",
    "    \n",
    "    return bench2_results,predictions, all_correctly_predicted    \n",
    "    \n",
    "def getRekd_v3Optimized(model, res, predictions, hist_preds, test,test_rows,train,start_choices, \\\n",
    "               scanned_num = [],RETRAIN_GAP=1, n=0,START=0,LIM=1000,\\\n",
    "            HIST_BINS=100,NUM_THREADS=NUM_THREADS,pred_past = None):\n",
    "    \n",
    "    testcoo = test.tocoo()\n",
    "    traincoo = train.tocoo()\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    # need this to make sure old predictions dont show up again\n",
    "    # even when predictions have been updated\n",
    " \n",
    "    if n > 0:\n",
    "        \n",
    "        pp = pred_past.multiply(10)\n",
    "        \n",
    "        #first subtract out previous pred_coo\n",
    "        predictions = predictions + pp\n",
    "\n",
    "    \n",
    "    #get top per row\n",
    "    chosen_indexes = np.concatenate(top_n_idx_sparse(predictions.tocsr().multiply(-1), RETRAIN_GAP))\n",
    "        \n",
    "\n",
    "    chosen_test_rows = np.repeat(test_rows,RETRAIN_GAP)\n",
    "    \n",
    "    scanned_num.append(len(chosen_indexes))\n",
    "    #make matrix of predictions\n",
    "    pred_all = coo_matrix(([1]*len(chosen_indexes),(chosen_test_rows,chosen_indexes)),shape=test.shape)\n",
    "        \n",
    " \n",
    "\n",
    "    # filter for only the correctly predicted this time\n",
    "    pred_correct_coo = testcoo.multiply(pred_all)\n",
    "    \n",
    "    \"\"\" \n",
    "    if UNIQUE_IPS:\n",
    "        pred_correct_ips = list(set(coo_matrix(pred_correct_coo).row))\n",
    "        test_rows_l = list(test_rows)\n",
    "        #find the corresponding prediction rows...\n",
    "        pred_cor_rows = [test_rows_l.index(i) for i in pred_correct_ips]\n",
    "        #\"zero them out\"\n",
    "        predictions[:,1][pred_cor_rows,:] = 10\n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    #find how many were correct\n",
    "    correct = pred_correct_coo.count_nonzero()\n",
    "        \n",
    "    #log chosen port\n",
    "    for i in chosen_indexes:\n",
    "        chosen_port = inv_port_map[i]    \n",
    "        if chosen_port not in res:\n",
    "            res[chosen_port] = 0\n",
    "\n",
    "        res[chosen_port] +=1 \n",
    "\n",
    "    print(\"correct:\",correct)\n",
    "    print(\"Num Scanned: \",len(chosen_indexes) )\n",
    "    hitrate = correct/len(chosen_indexes)\n",
    "    #print(\"accuracy:\", accuracy)\n",
    "    print(\"Number unique ports Rec'd: \", len(res))\n",
    "    #print(res)\n",
    "\n",
    "    newtrain = pred_correct_coo + traincoo #+ pred_false_coo\n",
    "    minitrain = pred_correct_coo\n",
    "\n",
    "    \n",
    "    if pred_past is not None:\n",
    "        pred_all = pred_all + pred_past\n",
    "    \n",
    "    return pred_all,  pred_correct_coo, newtrain,minitrain,hitrate,correct,\\\n",
    "        res,start_choices, scanned_num,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getMppHitRate(num_ips_per_port,LIM):\n",
    "    temp_ipsAndPorts = num_ips_per_port.data\n",
    "    temp_ipsAndPorts.sort()\n",
    "    temp_ipsAndPorts = temp_ipsAndPorts[::-1]/LIM\n",
    "    return temp_ipsAndPorts\n",
    "    \n",
    "\n",
    "\n",
    "def hist_along_axis(all_args):\n",
    "        (arr,bins) = all_args\n",
    "        hist, edges = np.histogram(arr,bins)\n",
    "        return np.array(hist)\n",
    "\n",
    "def hist_scores_per_user(arr,bins):\n",
    "\n",
    "    # Chunks for the mapping (only a few chunks):\n",
    "    chunks = [(sub_arr, bins) for sub_arr in arr]\n",
    "\n",
    "\n",
    "    pool = multiprocessing.Pool()\n",
    "    individual_results = pool.map(hist_along_axis, chunks)\n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    #print(individual_results[:2])\n",
    "    return individual_results\n",
    "\n",
    "\n",
    "def unpack_index(all_args):\n",
    "        (arr,i) = all_args\n",
    "        if i == 0:\n",
    "            return []\n",
    "        return np.argpartition(arr, -i)[-i:]\n",
    "\n",
    "\n",
    "def parallel_maxy(arr,elems):\n",
    "    \"\"\"\n",
    "    Like numpy.apply_along_axis(), but takes advantage of multiple\n",
    "    cores.\n",
    "    \"\"\" \n",
    "\n",
    "    # Chunks for the mapping (only a few chunks):\n",
    "    chunks = [(sub_arr, i) for sub_arr,i in zip(arr,elems)]\n",
    "\n",
    "\n",
    "    pool = multiprocessing.Pool()\n",
    "    individual_results = pool.map(unpack_index, chunks)\n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    #print(individual_results[:2])\n",
    "    return individual_results\n",
    "\n",
    "\n",
    "\n",
    "##PLOTTING MODULES##\n",
    "\n",
    "def plotDiscovery(DATAPATH,frac_services, frac_ips):\n",
    "\n",
    "    df = pd.read_csv(DATAPATH+\"lzr01_host_discovery.csv\")\n",
    "    plt.plot([0]+df[\"rolling_sum\"],label=\"ips - mpp (lzr 65k)\")\n",
    "\n",
    "    df = pd.read_csv(DATAPATH+\"lzr01_service_discovery.csv\")\n",
    "    plt.plot([0]+df[\"rolling_sum\"],label=\"services - mpp (lzr 65k)\")\n",
    "\n",
    "    plt.plot([0]+frac_services,label=\"services - ML\")\n",
    "    plt.plot([0]+frac_ips,label=\"ips -ML\")\n",
    "    plt.ylabel(\"Fraction of IPs/Services Discovered\")\n",
    "    plt.xlabel(\"Number of Scans\")\n",
    "    plt.xlim((1,len(df[\"rolling_sum\"])))\n",
    "    plt.title(\"IPv4 Discovery-Dynamic Choice\")\n",
    "    plt.xscale('log')\n",
    "    #plt.xlim((-1,20))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plotHitrate(hitrate,LIM):\n",
    "\n",
    "    mpp_hitrate =  getMppHitRate(num_ips_per_port,LIM)\n",
    "    #df = pd.read_csv(DATAPATH+\"lzr001_hitrate.csv\")\n",
    "    plt.plot([0]+mpp_hitrate,label=\"hitrate- mpp, (lzr 65k)\")\n",
    "    plt.plot([0]+hitrate,label=\"hitrate- ML\")\n",
    "    plt.legend()\n",
    "    plt.xlim((1,len(mpp_hitrate)))\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Number of Scans\")\n",
    "    plt.ylabel(\"Hitrate %\")\n",
    "    plt.title(\"Bandwidth Saved\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plotNormServiceDiscovery(scanned_num,normed_services,LIM):\n",
    "    plt.plot(np.cumsum([1]*int(max(np.cumsum(np.array(scanned_num)/LIM)))),label=\"mpp, (lzr 65k)\")\n",
    "    plt.plot(np.cumsum(np.array(scanned_num)/LIM),normed_services,label=\"ML\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Scans\")\n",
    "    plt.ylabel(\"# Normalized\")\n",
    "    plt.title(\"Normalized Services Discovered\")\n",
    "    plt.show()\n",
    "    \n",
    "def plotGradientNormServiceDiscovery(scanned_num,normed_services,LIM):\n",
    "\n",
    "    plt.plot([1]*int(max(np.cumsum(np.array(scanned_num)/LIM))),label=\"mpp, (lzr 65k)\")\n",
    "    plt.plot(np.cumsum(np.array(scanned_num)/LIM),np.gradient(normed_services,np.cumsum(np.array(scanned_num)/LIM)),label=\"ML\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of Scans\")\n",
    "    plt.ylabel(\"# Normalized Per Scan\")\n",
    "    plt.title(\"Derivative of Normalized Services Discovered\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df[\"ip\"] == inv_ip_map[0]][[\"asn\",\"p\",\"p1\",\"p2\",\"p3\"]])\n",
    "for i in list(df[df[\"ip\"] == inv_ip_map[0]][\"p\"]):\n",
    "    print(dmap[2][i])\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc = dmap[0][\"181.138.17.42\"]\n",
    "\n",
    "print(test[ipc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[ipc].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain with mini\n",
    "\n",
    "def trick(hybrid_model, port, weight):\n",
    "    user = dmap[0][\"2.59.42.8\"]\n",
    "    port = [dmap[2][port] ] #8894\n",
    "    d = ([user]*len(port),port)\n",
    "\n",
    "\n",
    "    #make matrix of predictions\n",
    "    mini_choice = coo_matrix(([1]*len(port),([user]*len(port),port)),shape=train.shape)\n",
    "    weights =  coo_matrix(([weight]*len(port),([user]*len(port),port)),shape=train.shape)\n",
    "\n",
    "    #newtrain = sparse.hstack((mat1, mat2))\n",
    "\n",
    "    #''' \n",
    "    hybrid_model = hybrid_model.fit_partial(mini_choice, #newtrain\n",
    "           sample_weight = weights,\n",
    "          user_features=user_features,\n",
    "          #item_features=item_features,\n",
    "          epochs=1,\n",
    "          num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "    #'''\n",
    "\n",
    "trick(hybrid_model,5223,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USERS = [0]\n",
    "PORTS = [37707,37717,37699,533,386,1271,0,23290]\n",
    "#print(PORTS)\n",
    "NUM_PREDICT = 20\n",
    "\n",
    "user_biases, user_embeddings = hybrid_model.get_user_representations(user_features[USERS])\n",
    "item_biases, item_embeddings = hybrid_model.get_item_representations()#item_features)\n",
    "\n",
    "print(user_biases)\n",
    "\n",
    "print(\"User Embeddings: \")\n",
    "print(user_embeddings)\n",
    "print(user_embeddings.dot(item_embeddings.T)[0][PORTS])\n",
    "\n",
    "predictions = (\n",
    "    user_embeddings.dot(item_embeddings.T) \n",
    "    +item_biases.reshape(1, -1) + user_biases.reshape(-1, 1)\n",
    ") \n",
    "\n",
    "print(\"Predictions:\")\n",
    "\n",
    "print(predictions[0][PORTS])\n",
    "print(\"----\")\n",
    "recd = predictions[0].argsort()[::-1][:NUM_PREDICT]\n",
    "for pid  in recd:\n",
    "    print(inv_port_map[pid])\n",
    "\n",
    "print(\"Item Biases: \")\n",
    "print(item_biases[PORTS])\n",
    "print(max(item_biases))\n",
    "\n",
    "print(\"----\")\n",
    "print(\"scores subtracted against item biased\")\n",
    "unbiased = user_embeddings.dot(item_embeddings.T)+ user_biases.reshape(-1, 1) \n",
    "print(unbiased[0][PORTS])\n",
    "unb_recd = unbiased[0].argsort()[::-1][:NUM_PREDICT]\n",
    "for pid  in unb_recd:\n",
    "    print(inv_port_map[pid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_items(item_id, item_features, model, N=10):\n",
    "    item_biases, item_representations = model.get_item_representations()#item_features)\n",
    "\n",
    "    # Cosine similarity\n",
    "    scores = item_representations.dot(item_representations[item_id, :])\n",
    "    item_norms = np.linalg.norm(item_representations, axis=1)\n",
    "    scores /= item_norms\n",
    "\n",
    "    best = np.argpartition(scores, -N)[-N:]\n",
    "    print(best)\n",
    "    return sorted(zip(best, scores[best] / item_norms[item_id]), \n",
    "                  key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "port_id = dmap[2][8893]\n",
    "items = similar_items(port_id,item_features,hybrid_model)\n",
    "\n",
    "for pid, sim in items:\n",
    "    print(inv_port_map[pid], sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis (Deep Dive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(coo):\n",
    "    coo = coo.tocoo(copy=False)\n",
    "    df_pred = pd.DataFrame({'ip': coo.row, 'port': coo.col})\n",
    "    df_pred[\"ip\"] = df_pred[\"ip\"].apply(lambda i: inv_ip_map[i] )\n",
    "    df_pred[\"port\"] = df_pred[\"port\"].apply(lambda i: inv_port_map[i] )\n",
    "    return df_pred\n",
    "\n",
    "#all_correctly_predicted = (newtrain - train)\n",
    "#testcoo = test.tocoo()\n",
    "#test_rows = list(set(testcoo.row))[START:LIM]\n",
    "\n",
    "#df_pred = convert(pred_all_past)\n",
    "#df_hit = convert(train)\n",
    "#df_hit.to_csv(DATAPATH + \"lzr1p_analysis/slash16ReTrainTrain.csv\")\n",
    "#df_missed = convert(test - all_correctly_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pysqldf(\"SELECT COUNT(distinct ip) FROM df_hit\"))\n",
    "print(pysqldf(\"SELECT COUNT(distinct ip) FROM df_missed\"))\n",
    "\n",
    "print(pysqldf(\"SELECT COUNT(*) FROM df_hit\"))\n",
    "print(pysqldf(\"SELECT COUNT(*) FROM df_missed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"p\"]==37443]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(a):\n",
    "    x, counts = np.unique(a, return_counts=True)\n",
    "    y = np.cumsum(counts)\n",
    "    x = np.insert(x, 0, x[0])\n",
    "    y = np.insert(y/y[-1], 0, 0.)\n",
    "    plt.plot(x, y, drawstyle='steps-post')\n",
    "    plt.xscale('log')\n",
    "\n",
    "    \n",
    "#For port 80 stuff that we are missing, how many ports do those ips usually respond on?\n",
    "num_missed = pysqldf(\" SELECT t1.ip, IFNULL(t2.c, 0) c FROM  \\\n",
    "(SELECT distinct ip FROM df_missed where port = 993) as t1 \\\n",
    "LEFT OUTER JOIN \\\n",
    "(SELECT ip, COUNT(distinct port)c FROM df_hit GROUP BY ip) as t2 \\\n",
    "ON t1.ip = t2.ip \")\n",
    "\n",
    "\n",
    "\n",
    "print(num_missed)\n",
    "ecdf(num_missed[\"c\"])#.hist(cumulative=True,histtype='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top ports we hit and top ports we miss\n",
    "\n",
    "#For port 80 stuff that we are missing, how many ports do those ips usually respond on?\n",
    "num_missed = pysqldf(\"SELECT port, COUNT(distinct ip)c FROM df_missed GROUP BY port \\\n",
    "ORDER BY c DESC \\\n",
    "LIMIT 20 \")\n",
    "print(num_missed)\n",
    "\n",
    "num_hit = pysqldf(\"SELECT port, COUNT(distinct ip)c FROM df_hit GROUP BY port \\\n",
    "ORDER BY c DESC \\\n",
    "LIMIT 20\")\n",
    "print(num_hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( pysqldf(\"SELECT COUNT(distinct asn) FROM df\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "https://stackoverflow.com/questions/37037450/multi-label-feature-selection-using-sklearn\n",
    "\n",
    "Check this out for using latent features in lightfm<br>\n",
    "https://github.com/lyst/lightfm/issues/486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pops(ports,df):\n",
    "    selected_features = [] \n",
    "\n",
    "    for p in ports:\n",
    "        X = df.drop([\"ip\"],axis=1)\n",
    "        Y = (X[\"p\"] == p )\n",
    "        X = X.drop([\"p\"],axis=1)\n",
    "        selector = SelectKBest(chi2, k='all')\n",
    "        selector.fit(X, Y)\n",
    "        selected_features.append(list(selector.scores_))  \n",
    "\n",
    "    return selected_features\n",
    "f = \"ht65k_miniset.csv\"\n",
    "df = pd.read_csv(DATAPATH+f)\n",
    "df = df.drop([\"minidata\"],axis=1)\n",
    "cols = ['slash20', 'slash19', 'slash18', 'slash17', 'slash16',\\\n",
    "        's1', 's2', 's3', 's4', 'asn'] #,'minidata']\n",
    "    \n",
    "df[cols] = df[cols].astype('category')\n",
    "df[cols] = df[cols].apply(lambda x: x.cat.codes)\n",
    "\n",
    "pop_ports = [80,443,7547,22,30005,21,8080,5060,4567,25,3306,110,3389,143,8089,8443,8008,587,8085,2000]\n",
    "\n",
    "rand_ports = list(set(df['p'].sample(n=100, random_state=1)))[:50]\n",
    "\n",
    "print(df.columns) \n",
    "\n",
    "print('popular ports')\n",
    "sf = find_pops(pop_ports, df)\n",
    "selected_features = np.mean(sf, axis=0)\n",
    "print(selected_features)\n",
    "\n",
    "\n",
    "print('random ports')\n",
    "sf = find_pops(rand_ports, df)\n",
    "selected_features = np.mean(sf, axis=0)\n",
    "print(selected_features)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeTest(f,tp,model_description, LR=0.01,N_COMP=4000,MAX_SAMPLE=175,EPOCHS=10,BIASED=False,\\\n",
    "                 RETRAIN=True,RETRAIN_WEIGHTS=False,WITH_ITEM_FEATS=False,CALC_WEIGHTS=False):\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    global test,pred_ranks,num_ips_per_port,test_rows,testcoo,inv_port_map\n",
    "    #f = \"ht65k_miniset.iconv\" # removed null byte! sed 's/\\x0//g' ht65k_1pset.csv > ht65k_1pset.iconv\n",
    "    #f = \"ht65k_1pset.iconv\"\n",
    "    \n",
    "    NUM_THREADS=72\n",
    "    \n",
    "    #Get the data\n",
    "    df, dataset, num_users, num_items, interactions, weights, \\\n",
    "    user_features,item_features, train,test,trainw,testw,dmap, inv_ip_map,\\\n",
    "    inv_port_map = extractData(f, train_percentage = tp, withItemFeats=WITH_ITEM_FEATS,calcWeights= CALC_WEIGHTS)\n",
    "\n",
    "\n",
    "    # Make the model\n",
    "    hybrid_model = LightFM(loss='warp', #'warp\n",
    "                    random_state=2016,\n",
    "                    learning_rate=LR, #0.05\n",
    "                    no_components=N_COMP,\n",
    "                    #learning_schedule='adadelta',\n",
    "                    max_sampled= MAX_SAMPLE, #50,\n",
    "                    #user_alpha=0.5   \n",
    "                          )\n",
    "\n",
    "    if WITH_ITEM_FEATS:\n",
    "        hybrid_model = hybrid_model.fit(train, \n",
    "                      user_features=user_features,\n",
    "                      item_features=item_features,\n",
    "                      sample_weight = trainw,\n",
    "                      epochs=EPOCHS, #10\n",
    "                      num_threads=NUM_THREADS, verbose=True)\n",
    "    else:\n",
    "        hybrid_model = hybrid_model.fit(train,\n",
    "                      user_features=user_features,\n",
    "                      sample_weight = trainw,\n",
    "                      epochs=EPOCHS, #10\n",
    "                      num_threads=NUM_THREADS, verbose=True)\n",
    "\n",
    "\n",
    "        \n",
    "    with open(DATAPATH+model_description+'_model.pickle', 'wb') as fle:\n",
    "        pickle.dump(hybrid_model, fle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    #CALL THE BENCHES\n",
    "    HIST_BINS = 100\n",
    "    START = 0\n",
    "    LIM = 200000  \n",
    "    BENCH1_PORTS = 100\n",
    "    BENCH2_CYCLES =25\n",
    "\n",
    "\n",
    "    num_test_ips, num_services,num_ips_per_port, test_rows, testcoo,\\\n",
    "        item_biases, item_embeddings, user_biases, user_embeddings, predictions, \\\n",
    "        pred_ranks, testPorts, hist_preds = \\\n",
    "        prepTheModel(hybrid_model,test,train,LIM,START,HIST_BINS,user_features,\\\n",
    "                     item_features,BIASED = BIASED,WITH_ITEM_FEATS=WITH_ITEM_FEATS)\n",
    " \n",
    "   \n",
    "\n",
    "    \n",
    "    np.random.seed(7)\n",
    "    randomPorts = np.random.choice(range(0,len(testPorts)), BENCH1_PORTS,replace=False)\n",
    "\n",
    "    TARGET_FRAC = 0.9\n",
    "    \"\"\" \n",
    "    bench1_Imrov_results = bench1_Improvements(np.array(testPorts)[randomPorts],TARGET_FRAC, LIM,\\\n",
    "                                              model_description, DATAPATH,\\\n",
    "                                              test,pred_ranks,num_ips_per_port,test_rows,testcoo,inv_port_map)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    bench2_Imrov_results = bench2_Improvements(hybrid_model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                                               test,train,user_features,item_features, \\\n",
    "                                               NUM_THREADS,RETRAIN,BIASED, LIM,HIST_BINS, BENCH2_CYCLES,\\\n",
    "                                               RETRAIN_WEIGHTS,WITH_ITEM_FEATS,model_description)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bench 1 Implementation:  Top Users Per Port to predict hitrate increase per port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImpr(r, label = \"\"):\n",
    "    improvements = []\n",
    "    for p in r:\n",
    "        temp = r[p][\"improvement\"]\n",
    "        if temp < 1:\n",
    "            temp = 1\n",
    "        improvements.append(temp)\n",
    "    \n",
    "    make_cdf(improvements, label)\n",
    "\n",
    "    \n",
    "def plotHitrates(DATAPATH, npy_fs, TARGET_FRAC):\n",
    "    for f in npy_fs:\n",
    "        res_og = np.load(DATAPATH +f ,allow_pickle=True).item()\n",
    "        plotImpr(res_og,f)\n",
    "        \n",
    "    #plt.legend()\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Hitrate Ratio Improvement\")\n",
    "    plt.ylabel(\"Fraction of Ports\")\n",
    "    plt.title(\"Hitrate(ML) / Hitrate(Random) to achieve \"+ str(TARGET_FRAC)+\" coverage\")\n",
    "    plt.show()\n",
    "\n",
    "def initializer():\n",
    "    global test,pred_ranks,num_ips_per_port,test_rows,testcoo,inv_port_map\n",
    "    \n",
    "# predicting hitrate increate per port compared to randomly probing\n",
    "def calcImprov(all_args):\n",
    "    (TPORT, TARGET_FRAC,LIM) = all_args\n",
    "    \n",
    "    res = {}\n",
    "    p = inv_port_map[TPORT]\n",
    "\n",
    "    target_total = len(test[test_rows,TPORT].data)\n",
    "    if target_total == 0:\n",
    "        return res\n",
    "    target_hit = num_ips_per_port.tocsr()[:,TPORT].data[0]/ LIM\n",
    "    #print(\"target port is: \", p)\n",
    "    #print(\"Target Total is: \", target_total)\n",
    "    #print(\"Target Hitrate is: \", target_hit)\n",
    "    \n",
    "    #all users where ranking = 1\n",
    "\n",
    "\n",
    "    scanned = []\n",
    "    old_proposed = 0\n",
    "    \n",
    "    # scan port by a time and scan most likely groups at a time\n",
    "    for rank in range(65535):\n",
    "        scores = np.array(list(pred_ranks[:,rank]))\n",
    "        ii = np.where(scores == TPORT)[0]\n",
    "        if len(ii) == 0:\n",
    "            continue\n",
    "        actual_test_rows = test_rows[ii]\n",
    "        scanned.extend(actual_test_rows)\n",
    "        \n",
    "        #if raw number is big enough, start checking correctness\n",
    "        if len(scanned) >= TARGET_FRAC*target_total:\n",
    "        #if True:\n",
    "        \n",
    "            if len(scanned) - old_proposed > 10000: #check 1k at a time\n",
    "                old_proposed = len(scanned)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            num_users = len(scanned)\n",
    "            chosen_indexes = [TPORT]* num_users\n",
    "\n",
    "            #make matrix of predictions\n",
    "            pred_all = coo_matrix(([1]*num_users,(scanned, chosen_indexes)),shape=test.shape)\n",
    "\n",
    "            # filter for only the correctly predicted this time\n",
    "            #pred_correct_coo = testcoo.multiply(pred_all)\n",
    "\n",
    "            combo = testcoo+pred_all\n",
    "            #find how many were correct\n",
    "            #correct = list(combo.data).count(2)\n",
    "            correct = sum(np.array(combo.data)-1)\n",
    "            fracFound = correct/target_total\n",
    "            hitrate = correct/num_users\n",
    "            improvement = hitrate/target_hit\n",
    "            \n",
    "            print(\"Improvement: \", improvement)\n",
    "            print(\"At rank: \",rank)\n",
    "            print(\"Scanned: \", len(scanned))\n",
    "            print(\"Correct: \", correct)\n",
    "            print(\"Hitrate: \",hitrate)\n",
    "            print(\"FracFound: \",fracFound)\n",
    "\n",
    "            \n",
    "            if fracFound >= TARGET_FRAC:\n",
    "                \n",
    "                res[p] = {\"rank\":rank, \"scanned\":len(scanned),\\\n",
    "                         \"correct\":correct, \"hitrate\":hitrate,\\\n",
    "                         \"fracFound\":fracFound,\"improvement\":improvement,\n",
    "                         \"target_total\":target_total, \"target_hit\":target_hit}\n",
    "                print(\"Improvement: \", improvement)\n",
    "                #print(\"At rank: \",rank)\n",
    "                #print(\"Scanned: \", len(scanned))\n",
    "                #print(\"Correct: \", correct)\n",
    "                #print(\"Hitrate: \",hitrate)\n",
    "                #print(\"FracFound: \",fracFound)\n",
    "                break\n",
    "            \n",
    "    return res\n",
    "\n",
    "    \n",
    "def bench1_Improvements(testPorts,TARGET_FRAC, LIM,model_description, DATAPATH,\\\n",
    "                       test,pred_ranks,num_ips_per_port,test_rows,testcoo,inv_port_map):\n",
    "    \n",
    "    \n",
    "    print(\"-------\")\n",
    "    print(\"...Beginning Bench1: Compare Hitrate To Random\")\n",
    "    # Chunks it up to parallelize\n",
    "    chunks = [(p,TARGET_FRAC,LIM) for p in testPorts]\n",
    "\n",
    "    pool = multiprocessing.Pool(30,initializer = initializer, initargs = ())\n",
    "    print(\"...pooled out calc\")\n",
    "    individual_results = pool.map(calcImprov, chunks)\n",
    "\n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    print(\"...calculations complete\")\n",
    "    result = {}\n",
    "    for d in individual_results:\n",
    "        result.update(d)\n",
    "        \n",
    "    np.save(DATAPATH + model_description+ \"_TARGETFRAC_\" + str(TARGET_FRAC)+\"_hitrateCompB1.npy\",result)\n",
    "    \n",
    "    npy_fs = [#\"hitrate_comp_item_user_feat.npy\",\\\n",
    "          #\"hitrate_comp_item_user_feat_weights_1kcap.npy\",\\\n",
    "             model_description+\"_TARGETFRAC_\" + str(TARGET_FRAC)+\"_hitrateCompB1.npy\"]\n",
    "\n",
    "    plotHitrates(DATAPATH, npy_fs, TARGET_FRAC)\n",
    "    \n",
    "    print(\"...Finished Bench1\")\n",
    "    print(\"-------\")\n",
    "    return result\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCoverage(model,predictions,hist_preds,num_services,num_test_ips,\\\n",
    "                 test,train,user_features, item_features, NUM_THREADS,RETRAIN,\\\n",
    "                 BIASED,LIM,HIST_BINS,\\\n",
    "                 CYCLES,RETRAIN_WEIGHTS,WITH_ITEM_FEATS):\n",
    "\n",
    "    \n",
    "    bench2_results = {}\n",
    "    portsScanned = {}\n",
    "    normed_services = []\n",
    "    frac_services = []\n",
    "    frac_ips = []\n",
    "    hitrate = []\n",
    "    scanned_num =  []\n",
    "    num_unique_rec = []\n",
    "    pred_all_past = None\n",
    "    start_choices = 1\n",
    "    newtrain = train.tocoo()\n",
    "    total_services_correct = 0\n",
    "                        \n",
    "    for i in range(1,CYCLES): \n",
    "        print(\"cycle: \",i)\n",
    "\n",
    "        if RETRAIN:\n",
    "            pred_all_past,  pred_correct, newtrain,minitrain,cur_hitrate,correct, \\\n",
    "            portsScanned,start_choices,scanned_num = \\\n",
    "            getRekd_v2(model,portsScanned, predictions, hist_preds, test,test_rows,\\\n",
    "                       newtrain,1,scanned_num,n=i,LIM=LIM,HIST_BINS=HIST_BINS,pred_past=pred_all_past)\n",
    "\n",
    "        else:\n",
    "            pred_all_past,  pred_correct, newtrain,minitrain,cur_hitrate,correct, \\\n",
    "            portsScanned,start_choices,scanned_num = \\\n",
    "            getRekd_v2(model,portsScanned, predictions, hist_preds, test,test_rows,\\\n",
    "                       newtrain,start_choices,scanned_num,n=i,LIM=LIM,HIST_BINS=HIST_BINS,pred_past=pred_all_past)\n",
    "\n",
    "        total_services_correct += correct                \n",
    "\n",
    "        print(\"At Bin: \", start_choices-1)\n",
    "        all_correctly_predicted = (newtrain - train).tocoo()\n",
    "        total_ips_correct = len(set(all_correctly_predicted.row))\n",
    "        print(\"total ips correct: \",total_ips_correct)    \n",
    "\n",
    "        num_correctly_pred_per_port = coo_matrix(all_correctly_predicted.sum(axis=0))\n",
    "\n",
    "        #do some kind of sum of all of these fractions and divide by total number ports?\n",
    "        norm_services = coo_matrix(num_correctly_pred_per_port/num_ips_per_port)\n",
    "        norm_services.data = np.nan_to_num(norm_services.data, copy=False)\n",
    "        norm_services = norm_services.sum()\n",
    "        \n",
    "        #if RETRAIN_WEIGHTS:\n",
    "        #    norm_services = norm_services/10\n",
    "        \n",
    "        print(\"Total Normalized Services: \", norm_services)\n",
    "        print(\"Total Fraction Services:\", total_services_correct/num_services)\n",
    "        print(\"Total Fraction IPs:\", total_ips_correct/num_test_ips)\n",
    "        print(\"Hitrate: \", cur_hitrate)\n",
    "        normed_services.append(norm_services)\n",
    "        frac_services.append(total_services_correct/num_services)\n",
    "        frac_ips.append(total_ips_correct/num_test_ips)\n",
    "        hitrate.append(cur_hitrate)\n",
    "        num_unique_rec.append(len(portsScanned))\n",
    "        \n",
    "        if RETRAIN:\n",
    "            \n",
    "            #scaling the retrain more\n",
    "            if RETRAIN_WEIGHTS:\n",
    "                minitrain.multiply(10)\n",
    "            \n",
    "            minitrainw = minitrain.tocoo()\n",
    "            \n",
    "            if WITH_ITEM_FEATS:\n",
    "                model = model.fit_partial(minitrain, \n",
    "                    sample_weight = minitrainw, #what to do about the weight???\n",
    "                    user_features=user_features,\n",
    "                    item_features=item_features,\n",
    "                    epochs=1,\n",
    "                    num_threads=NUM_THREADS, verbose=False)\n",
    "            else:\n",
    "                model = model.fit_partial(minitrain,\n",
    "                    sample_weight = minitrainw,\n",
    "                    user_features=user_features,\n",
    "                    epochs=1,\n",
    "                    num_threads=NUM_THREADS, verbose=False)       \n",
    "            \n",
    "             _ ,predictions= getNewPredictions(model,\\\n",
    "                    user_features,test_rows,item_features,BIASED,WITH_ITEM_FEATS)\n",
    "            hist_preds = hist_scores_per_user(predictions,100)\n",
    "            \n",
    "    \n",
    "    bench2_results[\"num_unique_rec\"] = num_unique_rec\n",
    "    bench2_results[\"frac_services\"] = frac_services\n",
    "    bench2_results[\"frac_ips\"] = frac_ips\n",
    "    bench2_results[\"hitrate\"] = hitrate\n",
    "    bench2_results[\"normed_services\"] = normed_services\n",
    "    bench2_results[\"scanned_num\"] = scanned_num\n",
    "    \n",
    "    return bench2_results    \n",
    "    \n",
    "def getRekd_v2(model, res, predictions, hist_preds, test,test_rows,train,start_choices, \\\n",
    "               scanned_num = [], n=1,START=0,LIM=1000,\\\n",
    "            HIST_BINS=100,NUM_THREADS=NUM_THREADS,pred_past = None):\n",
    "    \n",
    "    testcoo = test.tocoo()\n",
    "    traincoo = train.tocoo()\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    # need this to make sure old predictions dont show up again\n",
    "    # even when predictions have been updated\n",
    "    if n > 1:\n",
    "        \n",
    "        pp = np.array(pred_past.todense()[test_rows])\n",
    "        pp = pp*100\n",
    "        \n",
    "        #first subtract out previous pred_coo\n",
    "        predictions = predictions - pp\n",
    "  \n",
    "\n",
    "    temp_sum = 0\n",
    "    num_takers = [0]*len(hist_preds)\n",
    "    for i in range(start_choices,HIST_BINS): \n",
    "    \n",
    "        cur_num_takers = np.array(hist_preds)[:,-i]\n",
    "    \n",
    "        temp_sum += sum(cur_num_takers)\n",
    "        num_takers += cur_num_takers\n",
    "\n",
    "        if temp_sum > LIM:\n",
    "            start_choices = i+1\n",
    "            break\n",
    "            \n",
    "    scanned_num.append(temp_sum)\n",
    "    \n",
    "    chosen = parallel_maxy(predictions,num_takers)\n",
    "    \n",
    "    \n",
    "    found_test_rows = []\n",
    "    found_chosen_indexes = []\n",
    "    for trows, ports in zip(test_rows,chosen):\n",
    "        found_test_rows.extend([trows]*len(ports))\n",
    "        found_chosen_indexes.extend(ports)\n",
    "    \n",
    "\n",
    "    \n",
    "    #make matrix of predictions\n",
    "    pred_all = coo_matrix(([1]*len(found_test_rows),(found_test_rows,found_chosen_indexes)),shape=test.shape)\n",
    "        \n",
    "    # filter for only the correctly predicted this time\n",
    "    pred_correct_coo = testcoo.multiply(pred_all)\n",
    "    #pred_correct_coo = pred_correct_coo.multiply(10) #weight it\n",
    "        \n",
    "        \n",
    "    #filter for only incorrect and make them -1\n",
    "    #pred_false_coo = (pred_all - pred_correct_coo).multiply(-1)\n",
    "        \n",
    "    combo = testcoo+pred_all\n",
    "    #find how many were correct\n",
    "    correct = list(combo.data).count(2)\n",
    "        \n",
    "        \n",
    "    #log chosen port\n",
    "    for i in found_chosen_indexes:\n",
    "        chosen_port = inv_port_map[i]    \n",
    "        if chosen_port not in res:\n",
    "            res[chosen_port] = 0\n",
    "\n",
    "        res[chosen_port] +=1 \n",
    "\n",
    "    print(\"correct:\",correct)\n",
    "    print(\"Num Scanned: \",len(found_chosen_indexes) )\n",
    "    hitrate = correct/len(found_chosen_indexes)\n",
    "    #print(\"accuracy:\", accuracy)\n",
    "    print(\"Number unique ports Rec'd: \", len(res))\n",
    "    #print(res)\n",
    "\n",
    "\n",
    "    newtrain = pred_correct_coo + traincoo #+ pred_false_coo\n",
    "    minitrain = pred_correct_coo\n",
    "    \n",
    "    if pred_past is not None:\n",
    "        pred_all = pred_all + pred_past\n",
    "    \n",
    "    return pred_all,  pred_correct_coo, newtrain,minitrain,hitrate,correct,\\\n",
    "        res,start_choices, scanned_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the most likely port per user and scan all of them...\n",
    "# this does not handle stopping early for IPs which likely wont respond on anything\n",
    "# (i.e., wasted bandwitdth)\n",
    "# deprecated\n",
    "\n",
    "\n",
    "# https://github.com/inpefess/lightfm/blob/predict_comparison/\n",
    "# examples/batch_predict/predicting_with_matrix_multiplication.ipynb\n",
    "def getRekd(model,user_features, test,train,n=1,START=0,LIM=1000,\\\n",
    "            NUM_THREADS=NUM_THREADS,item_features=None,pred_past = None):\n",
    "    \n",
    "    \n",
    "    testcoo = test.tocoo()\n",
    "    traincoo = train.tocoo()\n",
    "\n",
    "    test_rows = list(set(testcoo.row))[START:LIM]\n",
    "    #test_rows.sort()\n",
    "    \n",
    "    if item_features is not None:\n",
    "        item_biases, item_embeddings = model.get_item_representations(item_features)\n",
    "    else:\n",
    "        item_biases, item_embeddings = model.get_item_representations()\n",
    "        \n",
    "    user_biases, user_embeddings = model.get_user_representations(user_features[test_rows])\n",
    "    \n",
    "    #print(\"this?\")\n",
    "    predictions = (\n",
    "        user_embeddings.dot(item_embeddings.T) +\n",
    "        item_biases.reshape(1, -1) + user_biases.reshape(-1, 1)\n",
    "    )\n",
    "    #print(\"yes\")\n",
    "    \n",
    "    print(predictions[3])\n",
    "   \n",
    "    res = {}\n",
    "    correct = 0\n",
    "\n",
    "    if n > 1:\n",
    "        \n",
    "        pp = np.array(pred_past.todense()[test_rows])\n",
    "        pp = pp*100\n",
    "        \n",
    "        #first subtract out previous pred_coo\n",
    "        predictions = predictions - pp\n",
    "\n",
    "    #what port IDs did we choose for first 10 \n",
    "    chosen_indexes = np.ravel(np.argmax(predictions,1))\n",
    "    #chosen_ratings = np.ravel(np.amax(predictions,1))\n",
    "    #print(\"n: \",n)\n",
    "    #print(chosen_indexes[:10])\n",
    "    \n",
    "    #make matrix of predictions\n",
    "    pred_all = coo_matrix(([1]*LIM,(test_rows,chosen_indexes)),shape=test.shape)\n",
    "        \n",
    "    # filter for only the correctly predicted this time\n",
    "    pred_correct_coo = testcoo.multiply(pred_all)\n",
    "        \n",
    "        \n",
    "    #filter for only incorrect and make them -1\n",
    "    pred_false_coo = (pred_all - pred_correct_coo).multiply(-1)\n",
    "        \n",
    "    combo = testcoo+pred_all\n",
    "    #find how many were correct\n",
    "    correct = list(combo.data).count(2)\n",
    "        \n",
    "        \n",
    "    #log chosen port\n",
    "    for i in chosen_indexes:\n",
    "        #print(i)\n",
    "        chosen_port = inv_port_map[i]    \n",
    "        if chosen_port not in res:\n",
    "            res[chosen_port] = 0\n",
    "\n",
    "        res[chosen_port] +=1 \n",
    "\n",
    "    print(\"correct:\",correct)\n",
    "    accuracy = correct/(n*(LIM))\n",
    "    #print(\"accuracy:\", accuracy)\n",
    "    print(\"Number unique ports Rec'd: \", len(res))\n",
    "    #print(res)\n",
    "\n",
    "    #how to add more info when predicting...\n",
    "    newtest =  None\n",
    "    newtrain = pred_correct_coo + traincoo + pred_false_coo\n",
    "    \n",
    "    if pred_past is not None:\n",
    "        pred_all = pred_all + pred_past\n",
    "    \n",
    "    return pred_all,  pred_correct_coo, newtrain,newtest, accuracy,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How to use depcreated Rekd method ^\n",
    "#with item features\n",
    "LIM = 10000\n",
    "START = 0\n",
    "\n",
    "\n",
    "num_test_ips, num_services,num_ips_per_port, test_rows = groundTruthServices(LIM,START)\n",
    "total_services_correct = 0\n",
    "\n",
    "\n",
    "    \n",
    "#adjust lim\n",
    "if LIM > len(test_rows):\n",
    "    LIM = len(test_rows)\n",
    "\n",
    "\n",
    "normed_services = []\n",
    "frac_services = []\n",
    "frac_ips = []\n",
    "hitrate = []\n",
    "\n",
    "for i in range(1,40): \n",
    "    print(\"cycle: \",i)\n",
    "    if i == 1:\n",
    "        pred_all_past,  pred_correct, newtrain,newtest, accuracy,correct =\\\n",
    "        getRekd(hybrid_model,user_features,test,train, n=i,LIM=LIM, item_features=item_features)\n",
    "    else:\n",
    "         pred_all_past,  pred_correct, newtrain,newtest,accuracy,correct =\\\n",
    "            getRekd(hybrid_model,user_features,test,newtrain,n=i,\\\n",
    "                    pred_past=pred_all_past,LIM=LIM,item_features=item_features)\n",
    "\n",
    "    total_services_correct += correct\n",
    "\n",
    "    ''' \n",
    "    hybrid_model = hybrid_model.fit_partial(newtrain, #newtrain\n",
    "      user_features=user_features,\n",
    "      item_features=item_features,\n",
    "      epochs=1,\n",
    "      num_threads=NUM_THREADS, verbose=False)\n",
    "\n",
    "    '''\n",
    "    all_correctly_predicted = (newtrain - train).tocoo()\n",
    "    total_ips_correct = len(set(all_correctly_predicted.row))\n",
    "    print(\"total ips correct: \",total_ips_correct)    \n",
    "        \n",
    "    #TODO: sum all_correctly_predicted\n",
    "    # divide by num_ips_per_port \n",
    "    # this may be easier by keeping as matrix\n",
    "    num_correctly_pred_per_port = coo_matrix(all_correctly_predicted.sum(axis=0))\n",
    "    \n",
    "    #do some kind of sum of all of these fractions and divide by total number ports?\n",
    "    norm_services = coo_matrix(num_correctly_pred_per_port/num_ips_per_port)\n",
    "    norm_services.data = np.nan_to_num(norm_services.data, copy=False)\n",
    "    norm_services = norm_services.sum()\n",
    "    cur_hitrate = correct/LIM\n",
    "    print(\"Total Normalized Services: \", norm_services)\n",
    "    print(\"Total Fraction Services:\", total_services_correct/num_services)\n",
    "    print(\"Total Fraction IPs:\", total_ips_correct/num_test_ips)\n",
    "    print(\"Hitrate: \", cur_hitrate)\n",
    "    normed_services.append(norm_services)\n",
    "    frac_services.append(total_services_correct/num_services)\n",
    "    frac_ips.append(total_ips_correct/num_test_ips)\n",
    "    hitrate.append(cur_hitrate)\n",
    "    \n",
    "    \n",
    "#TODO: accuracy for finding at least one service on an IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ips to be ports and visa versa to then be able to rec similar IPs\n",
    "# doesnt seem to work b/w too many items and data is too sparse\n",
    "def extractData_reverse(f, withItemFeats=True): \n",
    "    df_csv = pd.read_csv(DATAPATH+f) #, dtype={\"minidata\": str})\n",
    "    df_csv = df_csv.fillna(0)\n",
    "\n",
    "    #prune the items, as they are expensive\n",
    "    #if withItemFeats:\n",
    "    #    df_csv[\"minidata\"] = df_csv[\"minidata\"].astype('category')\n",
    "    #    df_csv[[\"minidata\"]] = df_csv[[\"minidata\"]].apply(lambda x: x.cat.codes)\n",
    "\n",
    "    #    df_csv['minidata'] = np.where(~df_csv['minidata'].duplicated(keep=False), 0, df_csv['minidata'])\n",
    "\n",
    "        \n",
    "        \n",
    "    #introduce dataset\n",
    "    dataset = Dataset()\n",
    "\n",
    "    dataset.fit(list(df_csv[\"p\"]),list(df_csv[\"ip\"]),\\\n",
    "            item_features=list(df_csv[\"slash20\"]))\n",
    "    #user_features = list(df_csv[\"ip\"])+list(df_csv[\"s1\"])+list(df_csv[\"s2\"])+list(df_csv[\"s3\"])+list(df_csv[\"s4\"])+\n",
    "\n",
    "    \n",
    "    num_users, num_items = dataset.interactions_shape()\n",
    "    print('Num users: {}, num_items {}.'.format(num_users, num_items))\n",
    "\n",
    "\n",
    "    #build interactions\n",
    "    (interactions, weights) = \\\n",
    "    dataset.build_interactions(list(zip(df_csv['p'], df_csv['ip'])))\n",
    "\n",
    "    print(repr(interactions))\n",
    "\n",
    "    #build featureset\n",
    "    newFeats = list(zip(df_csv['ip'], \\\n",
    "                        list(zip(list(df_csv[\"slash20\"])))))\n",
    "    item_features = dataset.build_item_features(newFeats)\n",
    "\n",
    "\n",
    "    train,test = cold_train_test_split_reverse(interactions,num_items,train_percentage=0.8)\n",
    "\n",
    "    #good to have mappings\n",
    "    dmap= dataset.mapping()\n",
    "    inv_port_map = {v: k for k, v in dmap[0].items()}\n",
    "    inv_ip_map = {v: k for k, v in dmap[2].items()}\n",
    "    \n",
    "\n",
    "    return df_csv, dataset, num_users, num_items, interactions,  None, item_features, train,test,dmap, inv_ip_map, inv_port_map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cold_train_test_split_reverse(interactions,num_items,train_percentage=0.5,random_state=7):\n",
    "    \n",
    "    \n",
    "    #remove weird duplicates that are introduced by lightfm\n",
    "    interactions_dok=dok_matrix((interactions.shape),dtype=interactions.dtype)\n",
    "    interactions_dok._update(zip(zip(interactions.row,interactions.col),interactions.data))\n",
    "\n",
    "    train_a = interactions_dok.tocsr()\n",
    "    test_a = interactions_dok.tocsr()\n",
    "    \n",
    "    \n",
    "    \n",
    "    indices_train = np.random.choice(np.arange(num_items), replace=False,\n",
    "                       size=int(num_items * train_percentage))\n",
    "    #how to get other indices\n",
    "    indices_test = list(set(np.arange(num_items)).difference(indices_train))\n",
    "    \n",
    "\n",
    "    X_train = lil_matrix(train_a[:,indices_train]).tocsr()\n",
    "    X_test = lil_matrix(test_a[:,indices_test]).tocsr()\n",
    "    \n",
    "    \n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#canonical ways to evaluate models...but these dont really mean much in our setting\n",
    "''' \n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(hybrid_model,train,user_features=user_features,num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid train AUC: %s' % train_auc)\n",
    "\n",
    "train_prec= precision_at_k(hybrid_model,train,k=1,user_features=user_features,num_threads=NUM_THREADS).mean()\n",
    "print(train_prec)\n",
    "\n",
    "\n",
    "# Compute and print the AUC score\n",
    "test_auc = auc_score(hybrid_model,test,train_interactions=train, user_features=user_features,num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid test AUC: %s' % test_auc)\n",
    "\n",
    "\n",
    "# Other Eval metrics\n",
    "train_prec= precision_at_k(hybrid_model,train,k=1,user_features=user_features,num_threads=NUM_THREADS).mean()\n",
    "print(train_prec)\n",
    "test_prec= precision_at_k(hybrid_model,test,k=1,user_features=user_features,num_threads=NUM_THREADS).mean()\n",
    "print(test_prec)\n",
    "'''\n",
    "\n",
    "#print(\"k=1\")\n",
    "#train_prec= recall_at_k(hybrid_model,train,k=1,user_features=user_features,num_threads=NUM_THREADS).mean()\n",
    "#print(train_prec)\n",
    "#test_prec= recall_at_k(hybrid_model,test,k=1,user_features=user_features,num_threads=NUM_THREADS).mean()\n",
    "#print(test_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to efficiently one hot encode...but also we dont really need this for this model \n",
    "# it 1 hot encodes itself\n",
    "f = \"ht65k_miniset.json\"\n",
    "df = pd.read_json(DATAPATH+f, lines=True)\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "print(\"df ready\")\n",
    "\n",
    "#somehow need to make this onehot automatically also have the ip labels?\n",
    "mlb = MultiLabelBinarizer()\n",
    "onehot = pd.DataFrame(mlb.fit_transform(df[\"p\"]),\n",
    "                   columns=mlb.classes_)\n",
    "\n",
    "\n",
    "print(\"onehot ready\")\n",
    "#onehot_csr = csr_matrix(onehot)\n",
    "df_ip = df[\"ip\"].astype('category')\n",
    "base_interactions = csr_matrix((np.ones(df_ip.shape[0]),(df_ip.ip.cat.codes,onehot.p.cat.codes)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
